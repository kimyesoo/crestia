#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pangea Reptile Blog Scraper
============================
Pangea Reptile ë¸”ë¡œê·¸ì—ì„œ í¬ë ˆìŠ¤í‹°ë“œ ê²Œì½” ê´€ë ¨ ì‚¬ìœ¡ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬
JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ìë™í™” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
-------
1. ì˜ì¡´ì„± ì„¤ì¹˜:
   pip install -r requirements.txt

2. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:
   python pangea_scraper.py

ì¶œë ¥:
-----
../src/constants/pangea_data.json
"""

import json
import os
import random
import re
import time
from datetime import datetime
from pathlib import Path

import requests
from bs4 import BeautifulSoup

# ============ ì„¤ì • ============
BASE_URL = "https://www.pangeareptile.com"
BLOG_URL = f"{BASE_URL}/blogs/blog"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
REQUEST_TIMEOUT = 30
SUMMARY_LENGTH = 200

# ì¶œë ¥ ê²½ë¡œ (ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ)
SCRIPT_DIR = Path(__file__).parent.resolve()
OUTPUT_DIR = SCRIPT_DIR.parent / "src" / "constants"
OUTPUT_FILE = OUTPUT_DIR / "pangea_data.json"


def get_soup(url: str) -> BeautifulSoup | None:
    """
    URLì—ì„œ HTMLì„ ê°€ì ¸ì™€ BeautifulSoup ê°ì²´ë¡œ ë°˜í™˜.
    ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ì‹œ None ë°˜í™˜.
    """
    headers = {"User-Agent": USER_AGENT}
    
    try:
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        return BeautifulSoup(response.text, "html.parser")
    except requests.RequestException as e:
        print(f"[ERROR] í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {url}")
        print(f"        {type(e).__name__}: {e}")
        return None


def clean_text(text: str) -> str:
    """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ."""
    if not text:
        return ""
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    return text.strip()


def extract_blog_list(soup: BeautifulSoup) -> list[dict]:
    """
    ë¸”ë¡œê·¸ ëª©ë¡ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ë§í¬ì™€ ì œëª© ì¶”ì¶œ.
    """
    articles = []
    
    # Shopify ë¸”ë¡œê·¸ êµ¬ì¡°ì— ë§ê²Œ ì…€ë ‰í„° ì¡°ì •
    # ë‹¤ì–‘í•œ ê°€ëŠ¥í•œ ì…€ë ‰í„° ì‹œë„
    selectors = [
        "article.article",
        ".blog-post",
        ".article-item",
        ".blog-article",
        "article",
        ".article"
    ]
    
    for selector in selectors:
        items = soup.select(selector)
        if items:
            print(f"[INFO] '{selector}' ì…€ë ‰í„°ë¡œ {len(items)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
            break
    else:
        # ëŒ€ì²´: ëª¨ë“  ë§í¬ì—ì„œ /blogs/blog/ íŒ¨í„´ ì°¾ê¸°
        print("[INFO] ê¸°ë³¸ ì…€ë ‰í„°ë¡œ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í•¨. ë§í¬ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰ ì¤‘...")
        all_links = soup.find_all("a", href=re.compile(r"/blogs/blog/[^/]+"))
        seen_urls = set()
        
        for link in all_links:
            href = link.get("href", "")
            if href and href not in seen_urls:
                seen_urls.add(href)
                title = clean_text(link.get_text()) or "Untitled"
                full_url = href if href.startswith("http") else f"{BASE_URL}{href}"
                articles.append({"title": title, "url": full_url})
        
        print(f"[INFO] ë§í¬ íŒ¨í„´ìœ¼ë¡œ {len(articles)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        return articles
    
    # ì¼ë°˜ì ì¸ ê²Œì‹œê¸€ ì¹´ë“œ íŒŒì‹±
    for item in items:
        link_tag = item.find("a", href=True)
        if not link_tag:
            continue
            
        href = link_tag.get("href", "")
        if not href or "/blogs/blog/" not in href:
            continue
            
        title_tag = item.find(["h1", "h2", "h3", "h4"]) or link_tag
        title = clean_text(title_tag.get_text()) if title_tag else "Untitled"
        
        full_url = href if href.startswith("http") else f"{BASE_URL}{href}"
        articles.append({"title": title, "url": full_url})
    
    return articles


def extract_article_content(soup: BeautifulSoup) -> str:
    """
    ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ.
    """
    # ë³¸ë¬¸ ì½˜í…ì¸ ë¥¼ ë‹´ëŠ” ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„
    content_selectors = [
        ".article__content",
        ".blog-content",
        ".rte",  # Shopify ê¸°ë³¸ Rich Text Editor í´ë˜ìŠ¤
        ".article-content",
        "article .content",
        ".post-content",
        "article"
    ]
    
    for selector in content_selectors:
        content_elem = soup.select_one(selector)
        if content_elem:
            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for tag in content_elem.find_all(["script", "style", "nav", "header", "footer"]):
                tag.decompose()
            
            text = clean_text(content_elem.get_text())
            if len(text) > 50:  # ìµœì†Œ 50ì ì´ìƒì˜ ë‚´ìš©ì´ ìˆì–´ì•¼ ìœ íš¨
                return text
    
    return ""


def scrape_pangea_blog() -> list[dict]:
    """
    Pangea Reptile ë¸”ë¡œê·¸ ì „ì²´ë¥¼ ìŠ¤í¬ë˜í•‘.
    """
    results = []
    
    print("=" * 60)
    print("ğŸ¦ Pangea Reptile Blog Scraper ì‹œì‘")
    print(f"   ëŒ€ìƒ: {BLOG_URL}")
    print("=" * 60)
    
    # 1. ë¸”ë¡œê·¸ ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
    print("\n[1/3] ë¸”ë¡œê·¸ ëª©ë¡ í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
    soup = get_soup(BLOG_URL)
    
    if not soup:
        print("[ERROR] ë¸”ë¡œê·¸ ëª©ë¡ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return results
    
    # 2. ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
    print("[2/3] ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ ì¤‘...")
    articles = extract_blog_list(soup)
    
    if not articles:
        print("[WARNING] ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return results
    
    print(f"[INFO] ì´ {len(articles)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
    
    # 3. ê° ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘
    print("[3/3] ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘ ì¤‘...")
    
    for i, article in enumerate(articles, 1):
        print(f"  [{i}/{len(articles)}] {article['title'][:40]}...")
        
        # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ë”œë ˆì´
        time.sleep(random.uniform(1, 3))
        
        try:
            detail_soup = get_soup(article["url"])
            
            if detail_soup:
                content = extract_article_content(detail_soup)
                summary = content[:SUMMARY_LENGTH] + "..." if len(content) > SUMMARY_LENGTH else content
                
                results.append({
                    "title": article["title"],
                    "url": article["url"],
                    "summary": summary,
                    "content": content,
                    "scraped_at": datetime.now().isoformat()
                })
                print(f"        âœ“ ìˆ˜ì§‘ ì™„ë£Œ ({len(content)}ì)")
            else:
                print(f"        âœ— ìƒì„¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"        âœ— ì—ëŸ¬ ë°œìƒ: {type(e).__name__}: {e}")
            continue
    
    return results


def save_to_json(data: list[dict]) -> bool:
    """
    ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥.
    """
    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ìœ¼ë©´)
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        output_data = {
            "source": "Pangea Reptile Blog",
            "source_url": BLOG_URL,
            "scraped_at": datetime.now().isoformat(),
            "total_articles": len(data),
            "articles": data
        }
        
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_FILE}")
        print(f"   ì´ {len(data)}ê°œ ê²Œì‹œê¸€ ì €ì¥ë¨")
        return True
        
    except Exception as e:
        print(f"\n[ERROR] íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜."""
    start_time = time.time()
    
    # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
    data = scrape_pangea_blog()
    
    if data:
        save_to_json(data)
    else:
        print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    elapsed = time.time() - start_time
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print("=" * 60)


if __name__ == "__main__":
    main()
