#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Korean Patch - íŒŒì¶©ë¥˜ ë°ì´í„° í•œêµ­ì–´ ë²ˆì—­ ìŠ¤í¬ë¦½íŠ¸
=================================================
ì˜ì–´ë¡œ ëœ ëª¨í”„ ë°ì´í„°ì™€ Pangea ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼
í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  SEO ìµœì í™”ëœ í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
-------
1. ì˜ì¡´ì„± ì„¤ì¹˜:
   pip install deep-translator

2. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:
   python korean_patch.py

ì¶œë ¥:
-----
../src/constants/morph_data_ko.json
../src/constants/pangea_data_ko.json
"""

import json
import re
import time
from datetime import datetime
from pathlib import Path

from deep_translator import GoogleTranslator

# ============ ê²½ë¡œ ì„¤ì • ============
SCRIPT_DIR = Path(__file__).parent.resolve()
CONSTANTS_DIR = SCRIPT_DIR.parent / "src" / "constants"

MORPH_INPUT = CONSTANTS_DIR / "morph_data.json"
MORPH_OUTPUT = CONSTANTS_DIR / "morph_data_ko.json"
PANGEA_INPUT = CONSTANTS_DIR / "pangea_data.json"
PANGEA_OUTPUT = CONSTANTS_DIR / "pangea_data_ko.json"

# ============ íŒŒì¶©ë¥˜ ì „ë¬¸ ìš©ì–´ ì‚¬ì „ (Glossary) ============
REPTILE_GLOSSARY = {
    # ì‚¬ìœ¡ ìš©ì–´
    "substrate": "ë°”ë‹¥ì¬",
    "Substrate": "ë°”ë‹¥ì¬",
    "shedding": "íƒˆí”¼",
    "Shedding": "íƒˆí”¼",
    "enclosure": "ì‚¬ìœ¡ì¥",
    "Enclosure": "ì‚¬ìœ¡ì¥",
    "terrarium": "í…Œë¼ë¦¬ì›€",
    "Terrarium": "í…Œë¼ë¦¬ì›€",
    "hatchling": "í•´ì¹­(ë² ì´ë¹„)",
    "Hatchling": "í•´ì¹­(ë² ì´ë¹„)",
    "juvenile": "ì¤€ì„±ì²´",
    "Juvenile": "ì¤€ì„±ì²´",
    "adult": "ì„±ì²´",
    "Adult": "ì„±ì²´",
    "gravid": "ë°°ë€(ì•Œì„ ë°´)",
    "Gravid": "ë°°ë€(ì•Œì„ ë°´)",
    "breeding": "ë¸Œë¦¬ë”©",
    "Breeding": "ë¸Œë¦¬ë”©",
    "clutch": "í´ëŸ¬ì¹˜(í•œ ë°° ì•Œ)",
    "Clutch": "í´ëŸ¬ì¹˜(í•œ ë°° ì•Œ)",
    
    # ìœ ì „ ìš©ì–´
    "morph": "ëª¨í”„",
    "Morph": "ëª¨í”„",
    "gene": "ìœ ì „ í˜•ì§ˆ",
    "Gene": "ìœ ì „ í˜•ì§ˆ",
    "recessive": "ì—´ì„±",
    "Recessive": "ì—´ì„±",
    "dominant": "ìš°ì„±",
    "Dominant": "ìš°ì„±",
    "incomplete dominant": "ë¶ˆì™„ì „ ìš°ì„±",
    "Incomplete Dominant": "ë¶ˆì™„ì „ ìš°ì„±",
    "co-dominant": "ê³µìš°ì„±",
    "Co-dominant": "ê³µìš°ì„±",
    "polygenic": "ë‹¤ìœ ì „ìì„±",
    "Polygenic": "ë‹¤ìœ ì „ìì„±",
    "heterozygous": "í—¤í…Œë¡œ(ì´í˜•ì ‘í•©)",
    "Heterozygous": "í—¤í…Œë¡œ(ì´í˜•ì ‘í•©)",
    "homozygous": "í˜¸ëª¨(ë™í˜•ì ‘í•©)",
    "Homozygous": "í˜¸ëª¨(ë™í˜•ì ‘í•©)",
    "phenotype": "í‘œí˜„í˜•",
    "Phenotype": "í‘œí˜„í˜•",
    "genotype": "ìœ ì „í˜•",
    "Genotype": "ìœ ì „í˜•",
    "lineage": "í˜ˆí†µ",
    "Lineage": "í˜ˆí†µ",
    "trait": "í˜•ì§ˆ",
    "Trait": "í˜•ì§ˆ",
    
    # ì¢… ì´ë¦„
    "crested gecko": "í¬ë ˆìŠ¤í‹°ë“œ ê²Œì½”",
    "Crested Gecko": "í¬ë ˆìŠ¤í‹°ë“œ ê²Œì½”",
    "gargoyle gecko": "ê°€ê³ ì¼ ê²Œì½”",
    "Gargoyle Gecko": "ê°€ê³ ì¼ ê²Œì½”",
    "leopard gecko": "ë ˆì˜¤íŒŒë“œ ê²Œì½”",
    "Leopard Gecko": "ë ˆì˜¤íŒŒë“œ ê²Œì½”",
    "chameleon": "ì¹´ë©œë ˆì˜¨",
    "Chameleon": "ì¹´ë©œë ˆì˜¨",
    
    # ì‚¬ìœ¡ í™˜ê²½
    "humidity": "ìŠµë„",
    "Humidity": "ìŠµë„",
    "temperature": "ì˜¨ë„",
    "Temperature": "ì˜¨ë„",
    "basking": "ë°”ìŠ¤í‚¹(ì¼ê´‘ìš•)",
    "Basking": "ë°”ìŠ¤í‚¹(ì¼ê´‘ìš•)",
    "UVB": "UVB",
    "misting": "ë¯¸ìŠ¤íŒ…(ë¶„ë¬´)",
    "Misting": "ë¯¸ìŠ¤íŒ…(ë¶„ë¬´)",
    "bioactive": "ë°”ì´ì˜¤ì•¡í‹°ë¸Œ",
    "Bioactive": "ë°”ì´ì˜¤ì•¡í‹°ë¸Œ",
    "isopod": "ì•„ì´ì†Œí¬ë“œ(ì¥ë©°ëŠë¦¬)",
    "Isopod": "ì•„ì´ì†Œí¬ë“œ(ì¥ë©°ëŠë¦¬)",
    "springtail": "í†¡í† ê¸°",
    "Springtail": "í†¡í† ê¸°",
    
    # ë¨¹ì´
    "diet": "ì‚¬ë£Œ",
    "Diet": "ì‚¬ë£Œ",
    "feeder": "ë¨¹ì´ê³¤ì¶©",
    "Feeder": "ë¨¹ì´ê³¤ì¶©",
    "cricket": "ê·€ëšœë¼ë¯¸",
    "Cricket": "ê·€ëšœë¼ë¯¸",
    "dubia": "ë‘ë¹„ì•„",
    "Dubia": "ë‘ë¹„ì•„",
    "gut-loading": "ê±°íŠ¸ë¡œë”©",
    "Gut-loading": "ê±°íŠ¸ë¡œë”©",
}

# ============ ëª¨í”„ ì´ë¦„ í•œê¸€í™” ì‚¬ì „ ============
MORPH_NAME_KO = {
    "Lilly White": "ë¦´ë¦¬ í™”ì´íŠ¸",
    "Axanthic": "ì•…ì‚°í‹±",
    "Dalmatian": "ë‹¬ë§ˆì‹œì•ˆ",
    "Super Dalmatian": "ìŠˆí¼ ë‹¬ë§ˆì‹œì•ˆ",
    "Harlequin": "í• ë¦¬í€¸",
    "Extreme Harlequin": "ìµìŠ¤íŠ¸ë¦¼ í• ë¦¬í€¸",
    "Pinstripe": "í•€ìŠ¤íŠ¸ë¼ì´í”„",
    "Flame": "í”Œë ˆì„",
    "Tiger": "íƒ€ì´ê±°",
    "Phantom": "íŒ¬í…€",
    "Cappuccino": "ì¹´í‘¸ì¹˜ë…¸",
    "Cream": "í¬ë¦¼",
    "Orange": "ì˜¤ë Œì§€",
    "Red Base": "ë ˆë“œ ë² ì´ìŠ¤",
    "Yellow Base": "ì˜ë¡œìš° ë² ì´ìŠ¤",
    "Black Base": "ë¸”ë™ ë² ì´ìŠ¤",
    "Lavender": "ë¼ë²¤ë”",
    "Tangerine": "íƒ ì €ë¦°",
    "Bi-color": "ë°”ì´ì»¬ëŸ¬",
    "Tri-color": "íŠ¸ë¼ì´ì»¬ëŸ¬",
    "Patternless": "íŒ¨í„´ë¦¬ìŠ¤",
    "Brindle": "ë¸Œë¦°ë“¤",
    "Halloween": "í• ë¡œìœˆ",
    "Cold Fusion": "ì½œë“œ í“¨ì „",
    "Snowflake": "ìŠ¤ë…¸ìš°í”Œë ˆì´í¬",
    "Hypo": "í•˜ì´í¬",
    "Sable": "ì„¸ì´ë¸”",
    "Empty Back": "ì— í‹°ë°±",
    "White Wall": "í™”ì´íŠ¸ ì›”",
    "Quad-stripe": "ì¿¼ë“œ ìŠ¤íŠ¸ë¼ì´í”„",
    "Super Stripe": "ìŠˆí¼ ìŠ¤íŠ¸ë¼ì´í”„",
    "Soft Scale": "ì†Œí”„íŠ¸ ìŠ¤ì¼€ì¼",
    "Chevron": "ì…°ë¸Œë¡ ",
    "Crowned": "í¬ë¼ìš´ë“œ",
    "Drippy": "ë“œë¦¬í”¼",
    "Fringing": "í”„ë¦°ì§•",
    "Furred": "í„ë“œ",
    "Ink Spot": "ì‰í¬ ìŠ¤íŒŸ",
    "Kneecaps": "ë‹ˆìº¡",
    "Monochrome": "ëª¨ë…¸í¬ë¡¬",
    "Oil Spot": "ì˜¤ì¼ ìŠ¤íŒŸ",
    "Olive": "ì˜¬ë¦¬ë¸Œ",
    "Portholes": "í¬íŠ¸í™€",
    "Solid Back": "ì†”ë¦¬ë“œ ë°±",
    "White Patterning": "í™”ì´íŠ¸ íŒ¨í„°ë‹",
    "Orange Patterning": "ì˜¤ë Œì§€ íŒ¨í„°ë‹",
    "White Tip": "í™”ì´íŠ¸ íŒ",
    "Cluster Spots": "í´ëŸ¬ìŠ¤í„° ìŠ¤íŒŸ",
    "Creamsicle": "í¬ë¦¼ì‹œí´",
    "Buckskin": "ë²…ìŠ¤í‚¨",
    "Blushing": "ë¸”ëŸ¬ì‹±",
    "Normal": "ë…¸ë§",
    "Tailless": "í…Œì¼ë¦¬ìŠ¤",
    "Hybrid": "í•˜ì´ë¸Œë¦¬ë“œ",
}

# ë²ˆì—­ê¸° ì´ˆê¸°í™”
translator = GoogleTranslator(source='en', target='ko')


def apply_glossary(text: str) -> str:
    """ì „ë¬¸ ìš©ì–´ ì‚¬ì „ì„ ì ìš©í•˜ì—¬ ë²ˆì—­ í’ˆì§ˆ í–¥ìƒ."""
    for eng, kor in REPTILE_GLOSSARY.items():
        text = text.replace(eng, kor)
    return text


def get_korean_morph_name(eng_name: str) -> str:
    """
    ì˜ì–´ ëª¨í”„ ì´ë¦„ì„ 'í•œê¸€ëª… (EngName)' í˜•ì‹ìœ¼ë¡œ ë³€í™˜.
    """
    # ì´ë¦„ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±° (ìœ ì „íƒ€ì…, ê°€ìš©ì„± ë“±)
    parts = eng_name.split()
    
    # ìœ ì „ íƒ€ì… ë° ê¸°íƒ€ í‚¤ì›Œë“œ ëª©ë¡
    stop_words = ['Recessive', 'Dominant', 'Incomplete', 'Polygenic', 'Other', 'Physical',
                  'Common', 'Average', 'Higher', 'Lower', 'Rarest', 'Availability',
                  'First', 'produced', 'in', 'At', 'least']
    
    # ì´ë¦„ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    name_parts = []
    for i, part in enumerate(parts):
        if part in stop_words:
            break
        # ì—°ë„ íŒ¨í„´ (2010, 2020 ë“±) ì œì™¸
        if re.match(r'^\d{4}$', part):
            break
        name_parts.append(part)
    
    clean_name = ' '.join(name_parts) if name_parts else eng_name.split()[0]
    
    # ì‚¬ì „ì—ì„œ í•œê¸€ ì´ë¦„ ì°¾ê¸°
    korean_name = MORPH_NAME_KO.get(clean_name, None)
    
    if korean_name:
        return f"{korean_name} ({clean_name})"
    else:
        # ì‚¬ì „ì— ì—†ìœ¼ë©´ ì˜ì–´ ì´ë¦„ ìœ ì§€
        return f"{clean_name}"


def translate_text(text: str, max_length: int = 4500) -> str:
    """
    í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­.
    ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë²ˆì—­.
    """
    if not text or len(text.strip()) == 0:
        return text
    
    try:
        # Google Translate ì œí•œìœ¼ë¡œ ê¸´ í…ìŠ¤íŠ¸ëŠ” ë¶„í• 
        if len(text) <= max_length:
            translated = translator.translate(text)
        else:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = text.replace('\n', ' [NEWLINE] ').split('. ')
            chunks = []
            current_chunk = ""
            
            for sentence in sentences:
                if len(current_chunk) + len(sentence) < max_length:
                    current_chunk += sentence + ". "
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + ". "
            
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # ê° ì²­í¬ ë²ˆì—­
            translated_chunks = []
            for chunk in chunks:
                time.sleep(0.5)  # API ì œí•œ ë°©ì§€
                translated_chunk = translator.translate(chunk)
                translated_chunks.append(translated_chunk)
            
            translated = ' '.join(translated_chunks)
            translated = translated.replace('[NEWLINE]', '\n')
        
        # ì „ë¬¸ ìš©ì–´ ì‚¬ì „ ì ìš©
        translated = apply_glossary(translated)
        
        return translated
        
    except Exception as e:
        print(f"[WARNING] ë²ˆì—­ ì‹¤íŒ¨: {str(e)[:50]}")
        return text


def process_morph_data():
    """ëª¨í”„ ë°ì´í„° í•œêµ­ì–´ ë³€í™˜."""
    print("\n" + "=" * 60)
    print("ğŸ§¬ ëª¨í”„ ë°ì´í„° í•œêµ­ì–´ ë³€í™˜ ì‹œì‘")
    print("=" * 60)
    
    if not MORPH_INPUT.exists():
        print(f"[ERROR] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MORPH_INPUT}")
        return False
    
    with open(MORPH_INPUT, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total = len(data['morphs'])
    translated_morphs = []
    
    for i, morph in enumerate(data['morphs'], 1):
        print(f"[{i}/{total}] {morph['name'][:40]}...")
        
        # ì´ë¦„ í•œê¸€í™”
        ko_name = get_korean_morph_name(morph['name'])
        
        # ì„¤ëª… ë²ˆì—­ (ìˆëŠ” ê²½ìš°)
        ko_description = ""
        if morph.get('description'):
            ko_description = translate_text(morph['description'])
            time.sleep(0.3)
        
        translated_morphs.append({
            "id": morph['id'],
            "name": ko_name,
            "nameEn": morph['name'],
            "type": morph.get('type', ''),
            "description": ko_description,
            "originalUrl": morph['originalUrl']
        })
    
    # ì¶œë ¥ ë°ì´í„° êµ¬ì„±
    output_data = {
        "source": "MorphMarket Morphpedia (í•œêµ­ì–´ ë²ˆì—­)",
        "source_url": data['source_url'],
        "translated_at": datetime.now().isoformat(),
        "translator": "Crestia Korean Patch",
        "total_morphs": len(translated_morphs),
        "morphs": translated_morphs
    }
    
    CONSTANTS_DIR.mkdir(parents=True, exist_ok=True)
    with open(MORPH_OUTPUT, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {MORPH_OUTPUT}")
    return True


def process_pangea_data():
    """Pangea ë¸”ë¡œê·¸ ë°ì´í„° í•œêµ­ì–´ ë³€í™˜."""
    print("\n" + "=" * 60)
    print("ğŸ“š Pangea ë¸”ë¡œê·¸ ë°ì´í„° í•œêµ­ì–´ ë³€í™˜ ì‹œì‘")
    print("=" * 60)
    
    if not PANGEA_INPUT.exists():
        print(f"[ERROR] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PANGEA_INPUT}")
        return False
    
    with open(PANGEA_INPUT, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total = len(data['articles'])
    translated_articles = []
    
    for i, article in enumerate(data['articles'], 1):
        print(f"[{i}/{total}] {article['title'][:40]}...")
        
        try:
            # ì œëª© ë²ˆì—­
            ko_title = translate_text(article['title'])
            time.sleep(0.3)
            
            # ìš”ì•½ ë²ˆì—­
            ko_summary = translate_text(article['summary'])
            time.sleep(0.3)
            
            # ë³¸ë¬¸ ë²ˆì—­ (ê¸´ í…ìŠ¤íŠ¸)
            ko_content = translate_text(article['content'])
            
            # ì¶œì²˜ ë¬¸êµ¬ ì¶”ê°€
            ko_content += "\n\n---\n> ğŸ”— ì¶œì²˜: Pangea Reptile Blog (í¬ë ˆìŠ¤í‹°ì•„ ë²ˆì—­)"
            
            translated_articles.append({
                "title": f"{ko_title}",
                "titleEn": article['title'],
                "url": article['url'],
                "summary": ko_summary,
                "content": ko_content,
                "scraped_at": article['scraped_at']
            })
            
            print(f"        âœ“ ë²ˆì—­ ì™„ë£Œ")
            time.sleep(1)  # API ì œí•œ ë°©ì§€
            
        except Exception as e:
            print(f"        âœ— ë²ˆì—­ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ìœ ì§€
            translated_articles.append({
                "title": article['title'],
                "titleEn": article['title'],
                "url": article['url'],
                "summary": article['summary'],
                "content": article['content'] + "\n\n---\n> ğŸ”— Source: Pangea Reptile Blog",
                "scraped_at": article['scraped_at']
            })
    
    # ì¶œë ¥ ë°ì´í„° êµ¬ì„±
    output_data = {
        "source": "Pangea Reptile Blog (í•œêµ­ì–´ ë²ˆì—­)",
        "source_url": data['source_url'],
        "translated_at": datetime.now().isoformat(),
        "translator": "Crestia Korean Patch",
        "total_articles": len(translated_articles),
        "articles": translated_articles
    }
    
    with open(PANGEA_OUTPUT, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {PANGEA_OUTPUT}")
    return True


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜."""
    start_time = time.time()
    
    print("=" * 60)
    print("ğŸ¦ Crestia Korean Patch - í•œêµ­ì–´ ë²ˆì—­ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)
    print("\n[INFO] ë¼ì´ë¸ŒëŸ¬ë¦¬: deep-translator (Google Translate API)")
    print("[INFO] ì „ë¬¸ ìš©ì–´ ì‚¬ì „ ì ìš©: í™œì„±í™”")
    
    # ëª¨í”„ ë°ì´í„° ì²˜ë¦¬
    process_morph_data()
    
    # Pangea ë°ì´í„° ì²˜ë¦¬
    process_pangea_data()
    
    elapsed = time.time() - start_time
    print("\n" + "=" * 60)
    print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed / 60:.1f}ë¶„")
    print("=" * 60)


if __name__ == "__main__":
    main()
