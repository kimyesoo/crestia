import time
import random
import json
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# ì €ì¥ ê²½ë¡œ ì„¤ì •
OUTPUT_DIR = "../src/constants"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "reptifiles_data.json")

# íƒ€ê²Ÿ URL
MAIN_URL = "https://reptifiles.com/crested-gecko-care/"

def setup_driver():
    """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
    options = Options()
    options.add_argument("--headless=new")  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # WebDriver ìë™ ì„¤ì¹˜ ë° ì„œë¹„ìŠ¤ ìƒì„±
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    return driver

def get_chapter_links(driver):
    """ë©”ì¸ í˜ì´ì§€ì—ì„œ ì±•í„° ë§í¬ ìˆ˜ì§‘"""
    print(f"ğŸ•µï¸  ë©”ì¸ í˜ì´ì§€ ë¶„ì„ ì¤‘... ({MAIN_URL})")
    
    try:
        driver.get(MAIN_URL)
        time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        # ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ì—ì„œ ë§í¬ ì°¾ê¸°
        links = []
        content_links = driver.find_elements(By.CSS_SELECTOR, ".entry-content a")
        
        for link in content_links:
            href = link.get_attribute("href")
            text = link.text.strip()
            
            # ìœ íš¨í•œ ì±•í„° ë§í¬ë§Œ í•„í„°ë§
            if (
                href and
                'reptifiles.com/crested-gecko-care/' in href and
                href != MAIN_URL and
                'share=' not in href and
                'jpg' not in href and
                '#' not in href and
                len(text) > 3
            ):
                if href not in [x['url'] for x in links]:
                    links.append({"title": text, "url": href})
        
        print(f"âœ… ì´ {len(links)}ê°œì˜ ìœ íš¨í•œ ì±•í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return links
        
    except Exception as e:
        print(f"âŒ ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {e}")
        return []

def get_chapter_content(driver, url):
    """ì±•í„° í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ"""
    try:
        print(f"   ğŸ“– Reading: {url[:60]}...")
        driver.get(url)
        time.sleep(random.uniform(2, 4))  # ëœë¤ ëŒ€ê¸°
        
        # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
        try:
            content_div = driver.find_element(By.CSS_SELECTOR, ".entry-content")
        except:
            print("   âš ï¸ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # JavaScriptë¡œ ê´‘ê³  ìš”ì†Œ ì œê±°
        driver.execute_script("""
            const selectors = ['.adthrive-ad', '.sharedaddy', '.jp-relatedposts', 
                               'script', 'style', '.widget', '.advertisement'];
            selectors.forEach(sel => {
                document.querySelectorAll(sel).forEach(el => el.remove());
            });
        """)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ (p, h2, h3, li íƒœê·¸)
        paragraphs = []
        elements = content_div.find_elements(By.CSS_SELECTOR, "p, h2, h3, li")
        
        for elem in elements:
            text = elem.text.strip()
            if len(text) > 1:
                tag_name = elem.tag_name.lower()
                if tag_name in ['h2', 'h3']:
                    paragraphs.append(f"\n## {text}\n")
                elif tag_name == 'li':
                    paragraphs.append(f"- {text}")
                else:
                    paragraphs.append(text)
        
        content = "\n\n".join(paragraphs)
        return content if content else None
        
    except Exception as e:
        print(f"   âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        return None

def main():
    # ì €ì¥ í´ë” í™•ì¸
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    print("=" * 60)
    print("ğŸ¦ ReptiFiles Crested Gecko Care Guide Scraper (Selenium)")
    print("=" * 60)
    
    # Selenium ë“œë¼ì´ë²„ ì‹œì‘
    print("\nğŸš€ Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
    driver = setup_driver()
    
    try:
        # ì±•í„° ë§í¬ ìˆ˜ì§‘
        links = get_chapter_links(driver)
        if not links:
            print("ìˆ˜ì§‘í•  ë§í¬ê°€ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        collected_data = []
        print("\nğŸ“š ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘ ì‹œì‘...")
        
        for idx, item in enumerate(links):
            print(f"\n[{idx+1}/{len(links)}] {item['title']} ìˆ˜ì§‘ ì¤‘")
            
            content = get_chapter_content(driver, item['url'])
            
            if content:
                collected_data.append({
                    "chapter": item['title'],
                    "url": item['url'],
                    "content": content
                })
                print(f"   âœ¨ ìˆ˜ì§‘ ì„±ê³µ ({len(content):,}ì)")
            else:
                print("   ğŸ§¨ ìˆ˜ì§‘ ì‹¤íŒ¨")
            
            # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ë”œë ˆì´
            delay = random.uniform(3, 6)
            time.sleep(delay)
        
        # JSON ì €ì¥
        output_data = {
            "source": "ReptiFiles",
            "source_url": MAIN_URL,
            "total_chapters": len(collected_data),
            "chapters": collected_data
        }
        
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'=' * 60}")
        print(f"ğŸ‰ ì™„ë£Œ! {len(collected_data)}ê°œì˜ ì±•í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ íŒŒì¼ ìœ„ì¹˜: {OUTPUT_FILE}")
        print("=" * 60)
        
    finally:
        driver.quit()
        print("ğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")

if __name__ == "__main__":
    main()
