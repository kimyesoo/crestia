#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MorphMarket Crested Gecko Morph Scraper (Selenium Version)
===========================================================
MorphMarket Morphpediaì—ì„œ í¬ë ˆìŠ¤í‹°ë“œ ê²Œì½” ëª¨í”„ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬
JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ìë™í™” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

Seleniumì„ ì‚¬ìš©í•˜ì—¬ JavaScriptë¡œ ë Œë”ë§ë˜ëŠ” ì½˜í…ì¸ ë„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
-------
1. ì˜ì¡´ì„± ì„¤ì¹˜:
   pip install -r requirements.txt

2. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:
   python morph_scraper.py

ì¶œë ¥:
-----
../src/constants/morph_data.json
"""

import json
import random
import re
import time
from datetime import datetime
from pathlib import Path

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# ============ ì„¤ì • ============
BASE_URL = "https://www.morphmarket.com"
MORPHPEDIA_URL = f"{BASE_URL}/morphpedia/crested-geckos/"
DESCRIPTION_MAX_LENGTH = 500

# ì¶œë ¥ ê²½ë¡œ (ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ)
SCRIPT_DIR = Path(__file__).parent.resolve()
OUTPUT_DIR = SCRIPT_DIR.parent / "src" / "constants"
OUTPUT_FILE = OUTPUT_DIR / "morph_data.json"


def create_driver():
    """Selenium Chrome ë“œë¼ì´ë²„ ìƒì„±."""
    options = Options()
    options.add_argument("--headless=new")  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    return driver


def clean_text(text: str) -> str:
    """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ."""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def slugify(name: str) -> str:
    """ì´ë¦„ì„ URL-safe IDë¡œ ë³€í™˜."""
    slug = name.lower()
    slug = re.sub(r'[^a-z0-9\s-]', '', slug)
    slug = re.sub(r'\s+', '-', slug)
    slug = re.sub(r'-+', '-', slug)
    return slug.strip('-')


def scrape_morphmarket(driver):
    """MorphMarket Morphpediaë¥¼ ìŠ¤í¬ë˜í•‘."""
    results = []
    
    print("=" * 60)
    print("ğŸ¦ MorphMarket Morphpedia Scraper ì‹œì‘ (Selenium)")
    print(f"   ëŒ€ìƒ: {MORPHPEDIA_URL}")
    print("=" * 60)
    
    # 1. Morphpedia ë©”ì¸ í˜ì´ì§€ ë¡œë“œ
    print("\n[1/3] Morphpedia ë©”ì¸ í˜ì´ì§€ ë¡œë”© ì¤‘...")
    try:
        driver.get(MORPHPEDIA_URL)
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ìµœëŒ€ 15ì´ˆ)
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        # ì¶”ê°€ ëŒ€ê¸° (JavaScript ë Œë”ë§)
        time.sleep(3)
    except Exception as e:
        print(f"[ERROR] í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
        return results
    
    # 2. í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ
    print("[2/3] í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
    last_height = driver.execute_script("return document.body.scrollHeight")
    scroll_count = 0
    max_scrolls = 10
    
    while scroll_count < max_scrolls:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height
        scroll_count += 1
        print(f"  ìŠ¤í¬ë¡¤ {scroll_count}íšŒ...")
    
    # 3. ëª¨í”„ ì¹´ë“œ ì¶”ì¶œ
    print("[3/3] ëª¨í”„ ì •ë³´ ì¶”ì¶œ ì¤‘...")
    soup = BeautifulSoup(driver.page_source, "html.parser")
    
    # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ ëª¨í”„ ì¹´ë“œ ì°¾ê¸°
    morph_cards = []
    
    # MorphMarketì˜ trait ì¹´ë“œ ì…€ë ‰í„°ë“¤
    selectors = [
        "a[href*='/morphpedia/crested-geckos/'][href$='/']",
        ".trait-card a",
        "[class*='TraitCard'] a",
        ".morph-item a",
        "a.trait-link",
    ]
    
    for selector in selectors:
        cards = soup.select(selector)
        if cards:
            print(f"[INFO] '{selector}' ì…€ë ‰í„°ë¡œ {len(cards)}ê°œ ìš”ì†Œ ë°œê²¬")
            morph_cards.extend(cards)
            break
    
    if not morph_cards:
        # ëŒ€ì²´: ëª¨ë“  ëª¨í”„ ê´€ë ¨ ë§í¬ ì°¾ê¸°
        print("[INFO] ê¸°ë³¸ ì…€ë ‰í„°ë¡œ ëª¨í”„ë¥¼ ì°¾ì§€ ëª»í•¨. ë§í¬ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰ ì¤‘...")
        all_links = soup.find_all("a", href=re.compile(r"/morphpedia/crested-geckos/[a-z0-9-]+/?$", re.I))
        morph_cards = [link for link in all_links if link.get("href") != "/morphpedia/crested-geckos/"]
        print(f"[INFO] ë§í¬ íŒ¨í„´ìœ¼ë¡œ {len(morph_cards)}ê°œ ëª¨í”„ ë°œê²¬")
    
    # ì¤‘ë³µ ì œê±°
    seen_urls = set()
    unique_morphs = []
    
    for card in morph_cards:
        href = card.get("href", "")
        if not href or href in seen_urls:
            continue
        if "/morphpedia/crested-geckos/" not in href:
            continue
        if href == "/morphpedia/crested-geckos/":
            continue
            
        seen_urls.add(href)
        
        # ì´ë¦„ ì¶”ì¶œ
        name = clean_text(card.get_text())
        if not name or len(name) < 2:
            # hrefì—ì„œ ì´ë¦„ ì¶”ì¶œ
            name_from_url = href.rstrip("/").split("/")[-1]
            name = name_from_url.replace("-", " ").title()
        
        full_url = href if href.startswith("http") else f"{BASE_URL}{href}"
        unique_morphs.append({"name": name, "url": full_url})
    
    print(f"[INFO] ì´ {len(unique_morphs)}ê°œ ê³ ìœ  ëª¨í”„ ë°œê²¬")
    
    # 4. ê° ëª¨í”„ ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸
    for i, morph in enumerate(unique_morphs, 1):
        print(f"  [{i}/{len(unique_morphs)}] {morph['name'][:30]}...")
        
        try:
            driver.get(morph["url"])
            time.sleep(random.uniform(1.5, 3))
            
            detail_soup = BeautifulSoup(driver.page_source, "html.parser")
            
            # ì„¤ëª… ì¶”ì¶œ
            description = ""
            desc_selectors = [
                ".trait-description",
                ".description",
                "[class*='Description']",
                ".content p",
                "article p",
                "main p"
            ]
            
            for selector in desc_selectors:
                desc_elem = detail_soup.select_one(selector)
                if desc_elem:
                    text = clean_text(desc_elem.get_text())
                    if len(text) > 30:
                        description = text[:DESCRIPTION_MAX_LENGTH]
                        if len(text) > DESCRIPTION_MAX_LENGTH:
                            description += "..."
                        break
            
            # íƒ€ì… ì¶”ì¶œ (Dominant, Recessive ë“±)
            morph_type = ""
            type_selectors = [
                ".trait-type",
                ".inheritance",
                "[class*='Type']",
                ".badge",
                ".tag"
            ]
            
            for selector in type_selectors:
                type_elem = detail_soup.select_one(selector)
                if type_elem:
                    type_text = clean_text(type_elem.get_text())
                    if type_text and len(type_text) < 50:
                        morph_type = type_text
                        break
            
            results.append({
                "id": slugify(morph["name"]),
                "name": morph["name"],
                "type": morph_type,
                "description": description,
                "originalUrl": morph["url"]
            })
            
            print(f"        âœ“ ìˆ˜ì§‘ ì™„ë£Œ - {morph_type or 'Unknown Type'}")
            
        except Exception as e:
            print(f"        âœ— ì—ëŸ¬: {type(e).__name__}: {e}")
            results.append({
                "id": slugify(morph["name"]),
                "name": morph["name"],
                "type": "",
                "description": "",
                "originalUrl": morph["url"]
            })
            continue
    
    return results


def save_to_json(data: list[dict]) -> bool:
    """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥."""
    try:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        output_data = {
            "source": "MorphMarket Morphpedia",
            "source_url": MORPHPEDIA_URL,
            "scraped_at": datetime.now().isoformat(),
            "total_morphs": len(data),
            "morphs": data
        }
        
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_FILE}")
        print(f"   ì´ {len(data)}ê°œ ëª¨í”„ ì €ì¥ë¨")
        return True
        
    except Exception as e:
        print(f"\n[ERROR] íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜."""
    start_time = time.time()
    driver = None
    
    try:
        print("[INIT] Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
        driver = create_driver()
        
        # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        data = scrape_morphmarket(driver)
        
        if data:
            save_to_json(data)
        else:
            print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"\n[FATAL] ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
    finally:
        if driver:
            driver.quit()
            print("[CLEANUP] ë“œë¼ì´ë²„ ì¢…ë£Œë¨")
    
    elapsed = time.time() - start_time
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print("=" * 60)


if __name__ == "__main__":
    main()
